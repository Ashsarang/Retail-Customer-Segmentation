# -*- coding: utf-8 -*-
"""ML Online Retail Customer Segmentation by Ashish Sarang .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TocOLXu3abvGOVbMA-uPJv6nWG4UsBMO

## **Project Name**    - **Online Retail Customer Segmentation**

##### **Project Type**    - Unsupervised
##### **Contribution**    - Individual

# **Project Summary -**

Customer segmentation is a crucial strategy in business and marketing, involving the division of a broad customer base into distinct groups based on shared characteristics or behaviors. This approach enables businesses to gain a deeper understanding of their customers, address their needs more effectively, and optimize various aspects of their operations. As a result, companies can achieve enhanced marketing efficiency, higher customer satisfaction, and improved overall business performance.

# **GitHub Link -**

Provide your GitHub Link here.

# **Problem Statement**

In this project, your task is to identify major customer segments within a transnational data set encompassing all transactions from 01/12/2010 to 09/12/2011 for a UK-based, non-store online retailer. The company primarily sells unique, all-occasion gifts, with many of its customers being wholesalers.

# **General Guidelines** : -

1.   Well-structured, formatted, and commented code is required.
2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.
     
     The additional credits will have advantages over other students during Star Student selection.
       
             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go
                       without a single error logged. ]

3.   Each and every logic should have proper comments.
4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.
        

```
# Chart visualization code
```
            

*   Why did you pick the specific chart?
*   What is/are the insight(s) found from the chart?
* Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

5. You have to create at least 15 logical & meaningful charts having important insights.


[ Hints : - Do the Vizualization in  a structured way while following "UBM" Rule.

U - Univariate Analysis,

B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)

M - Multivariate Analysis
 ]





6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.


*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.


*   Cross- Validation & Hyperparameter Tuning

*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.

# ***Let's Begin !***

## ***1. Know Your Data***

### Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# Import Libraries
import pandas as pd       # Library for data analysis
import numpy as np          # Basic mathematic operation

import matplotlib.pyplot as plt     # Library for visualization
import matplotlib.cm as cm            # Tool for enhancing data visualizations through effective use of color
import seaborn as sns             # High level interface for drawing a informative staticals graph.
# %matplotlib inline
import missingno as msno          # Library for visualize the missing Data.

from datetime import datetime     # For Handling Datatime data
import datetime as dt
from numpy import math

import scipy.stats as stats

from sklearn.preprocessing import StandardScaler   # Standardize features in dataset.
from sklearn.cluster import KMeans            # Importing kMeans ML model
from sklearn.cluster import DBSCAN            # Importing Dbscan ML model
from sklearn.metrics import silhouette_samples, silhouette_score

import warnings
warnings.filterwarnings('ignore')

"""### Dataset Loading"""

from google.colab import drive
drive.mount('/content/drive')
df = pd.read_csv("/content/drive/MyDrive/Alma Better EDA files/Online Retail.xlsx - Online Retail.csv")

"""### Dataset First View"""

# First five rows
df.head()

# last Five rows
df.tail()

"""### Dataset Rows & Columns count"""

# Dataset Rows & Columns count
print(f" Number of Rows: {df.shape[0]} ")
print(f"Number of columns: {df.shape[1]}")

"""### Dataset Information"""

# Dataset Info
df.info()

"""#### Duplicate Values"""

# Dataset Duplicate Value Count
df[df.duplicated()]

"""#### Missing Values/Null Values"""

# Missing Values/Null Values Count
df.isnull().sum()

# Visualizing the missing values
msno.bar(df, figsize=(6,4),color="seagreen",fontsize=12)

"""### What did you know about your dataset?

*   This dataset comprises 541,909 rows and 8 columns.
*   There are 5,268 duplicate rows.
*   The columns "Description" and "customer_id" contain null values

## ***2. Understanding Your Variables***
"""

# Dataset Columns
df.columns

# Dataset Describe
df.describe()

"""### Variables Description

InvoiceNo: Invoice number. A 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.

StockCode: Product (item) code— a 5-digit integral number uniquely assigned to each distinct product.

Description: Product (item) name.

Quantity: The quantities of each product (item) per transaction.

InvoiceDate: Invoice date and time.

Unit Price: Product price per unit.

CustomerID: Customer number, a 5-digit integral number uniquely assigned to each customer.

Country: Country name. The name of the country where each customer resides.

### Check Unique Values for each variable.
"""

# Check Unique Values for each variable.
for i in df.columns:
  unique = df[i].unique()
  print(f"Unique value for {i}: {unique}")
  print("--"*50)

"""## 3. ***Data Wrangling***

### Data Wrangling Code
"""

# Write your code to make your dataset analysis ready.
# Droping the null values
df.dropna(inplace=True)

# checking null values
df.isnull().any()

# changing datatype
df['InvoiceNo'] = df['InvoiceNo'].astype('str')

"""# Here we dropped some InvoiceNo which starts with 'c' because 'c' indicates a cancellation."""

# droping the rows which contain c
df = df[~df['InvoiceNo'].str.contains('C')]

# droping the duplicate values
df.drop_duplicates(inplace = True)

# creating new features TotalAmount
df['TotalAmount'] = df['Quantity'] * df['UnitPrice']

# converting InvoiceDate column into datetime format
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%d/%m/%y %H:%M', errors='coerce')

# creating new column from InvoiceDate
df['Month'] = df['InvoiceDate'].dt.month_name()
df['Day']   = df['InvoiceDate'].dt.day_name()
df['Hour']  = df['InvoiceDate'].dt.hour

df.shape

"""### What all manipulations have you done and insights you found?

*   I have removed the null values present in the "Description" and "Customer" columns.
*   In the "InvoiceNo" column, "C" indicates a cancellation; therefore, I have removed the rows where "InvoiceNo" contains "C".
*   I have also removed the duplicate values present in our dataset.
*   I created a new feature, "TotalAmount," by multiplying "Quantity" and "UnitPrice."
*   I have also created three new columns: "Month," "Day," and "Hour" from the "InvoiceDate" column.

## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***

Chart - 1 **Top 10 Highest spending customer**.
"""

# Chart - 1 visualization code
top_cust = df.groupby(['CustomerID'])['TotalAmount'].sum().reset_index()
top_10_cust = top_cust.sort_values('TotalAmount', ascending=False).head(10)
print(top_10_cust)

plt.figure(figsize=(10,8)) # Graph size
sns.set_style('darkgrid') # Style of Grpah
# Bar plot
sns.barplot(x = 'CustomerID', y = 'TotalAmount', data = top_10_cust)
#Title of Graph
plt.title("Highest Spending Customer",color='navy')
# Labels of Graph
plt.xlabel("Customer ID",color = 'blue')
plt.ylabel("Total Amount Spend",color='blue')
# Showing Graph
plt.show()

"""##### 1. Why did you pick the specific chart?

Bar charts are great for comparing different categories.

##### 2. What is/are the insight(s) found from the chart?

*   The top spenders are revealed in this graph, with the ten highest-value customers identified.
*   customer ID 14646 and 18102 spend most.

#### Chart - 2 **Top 10 Product by Unit Counts**
"""

description = df['Description'].value_counts().reset_index()
description_df = description.head(10)
print(description_df)

plt.figure(figsize=(12,8))   # Graph size

sns.set_style("whitegrid")  # Style of Graph

sns.barplot(data = description_df, x = 'count', y = 'Description',palette = 'viridis')
# Title of Graph
plt.title("Top Product by Unit Count", color='navy')
# Label of Graph
plt.xlabel("Number of Product",color='blue')
plt.ylabel("Product Name", color='blue')
# Showing Grpah
plt.show()

"""##### 1. Why did you pick the specific chart?

The best tool for displaying comparisons between several categories of data. Horizontal rectangles leave enough room for textual information and are therefore ideal for long data labels.

##### 2. What is/are the insight(s) found from the chart?

This graph reveals our best-selling product.

#### Chart - 3 **Top 10 Best Selling products.**
"""

top_product = df.groupby(['Description'])['TotalAmount'].sum().sort_values(ascending=False).reset_index().head(10)
print(top_product)

plt.figure(figsize=(10,8))
sns.set_style('darkgrid')

sns.barplot(x = 'TotalAmount', y= 'Description',data = top_product, palette="rocket")
# Graph title
plt.title(" Best Selling Products", color='navy')
# Graph lebel
plt.xlabel("Total Amount", color='blue')
plt.ylabel("Product Description", color='blue')

plt.show()

"""##### 2. What is/are the insight(s) found from the chart?

*   This visualization shows which product generated the highest sales revenue.
*   Interestingly, while the graph shows "WHITE HANGING HEART T-LIGHT HOLDER" as the top seller in terms of units sold, " PAPER CRAFT , LITTLE BIRDIE " generated the highest total sales revenue.

#### Chart - 4 **Top 10 Customers with Most Frequent Purchases**
"""

customer_purchases_frequent = df['CustomerID'].value_counts().sort_values(ascending= False).reset_index()
top_frequent_purchaser_df = customer_purchases_frequent.head(10)
print(top_frequent_purchaser_df)

plt.figure(figsize=(10,6))  # Graph size
sns.set_style("darkgrid")   # Graph style

sns.barplot(x= 'CustomerID', y= 'count', palette= "Paired", data= top_frequent_purchaser_df)

# Setting graph title and labels
plt.title("Customers with Most Frequent Purchases",color='navy')
plt.xlabel("CustomerId", color='blue')
plt.ylabel("Purchases Frequency",color='blue')
# Showing the graph
plt.show()

"""##### 2. What is/are the insight(s) found from the chart?

*   The top customers in terms of purchase frequency are revealed in this graph.
*   We can likewise says that they are most faithful clients.

#### Chart - 5 **Top 5 Countries with the Most Potential Customers**
"""

country_customer_count = df['Country'].value_counts().sort_values(ascending=False).reset_index()
country_customer_count_df = country_customer_count.head(5)
print(country_customer_count_df)

plt.figure(figsize=(10,6))

sns.barplot(x = 'count', y='Country',data = country_customer_count_df, palette='magma')
# setting title and labels of Graph.
plt.title("Country with the Most Potential Customers",color='navy')
plt.xlabel("Count of Customers",color='blue')
plt.ylabel("Country Name",color='blue')
# showing graph
plt.show()

"""##### 2. What is/are the insight(s) found from the chart?

*   The largest number of customers are from the UK, followed by Germany and France.

#### Chart - 6 **Country with the Highest Customer Expenditure.**
"""

country_expenditure_df = df.groupby(['Country'])['TotalAmount'].sum().sort_values(ascending=False).reset_index().head()
print(country_expenditure_df)

plt.figure(figsize=(10,6))
sns.set_style('darkgrid')
sns.barplot(x = 'Country', y='TotalAmount',data = country_expenditure_df, palette='bright')
# setting title and labels of Graph.
plt.title("Top 5 Counrty witht he Highest Customer Expenditure",color='navy')
plt.xlabel("Country Name",color='blue')
plt.ylabel("Total Expenditure",color='blue')
plt.show()

"""##### 2. What is/are the insight(s) found from the chart?

The top spending customers are primarily from the UK, followed by those from the Netherlands and EIRE.

#### Chart - 7 **Monthly Trend in Total Spending**
"""

monthly_spending = df.groupby(['Month'])['TotalAmount'].sum().reset_index()
print(monthly_spending)

plt.figure(figsize=(12,8))
# Line Plot
plt.plot(monthly_spending['Month'],  monthly_spending['TotalAmount'], marker='o',mec='red',mfc='green',ms=8)
# Graph Title
plt.title("Monthy Trend in Total Spending",color='navy')
# Graph Labels
plt.xlabel("Month Name",color='blue')
plt.ylabel("Total Amount Spend",color='blue')
plt.show()

"""##### 1. Why did you pick the specific chart?

Line graphs are excellent for showing trends over time. By plotting Total amount spend against months, you can easily observe how the values change from one month to the next.

##### 2. What is/are the insight(s) found from the chart?

*   Most numbers of customers have purchased the gifts in the month of November, October and December.As we all know they have festive season in end of the year as well new year to celebrate so we have highest numbers of transaction in november, october, december as company have most of the customer wholesales who are keeping stocks for festive season.
*   Least numbers of purchasing are in the month of April and February.

#### Chart - 8 **Daily Transaction Volume.**
"""

day_df = df['Day'].value_counts().reset_index()
day_df

plt.figure(figsize=(8,6))
# Line Plot
plt.plot(day_df['Day'], day_df['count'], marker='*',mec='red',ms=10)
# Graph Title
plt.title("Daily Transaction Volume ",color='navy')
# Graph Labels
plt.xlabel("Day Name",color='blue')
plt.ylabel("Number of transaction",color='blue')
plt.show()

"""##### 2. What is/are the insight(s) found from the chart?

*   We can see the maximum number of transaction are for thursday but we can also see there are no transaction on saturday

#### Chart - 9 **Hourly Transaction Volume**
"""

hour_df = df.groupby(['Hour'])['TotalAmount'].sum().reset_index()
print(hour_df)

plt.figure(figsize=(12,8))
sns.barplot(x = 'Hour', y = 'TotalAmount', data = hour_df, palette = 'viridis')
# Title of Graph
plt.title("Hourly Transaction Volume",color='navy')
# Labels of Grpah
plt.xlabel("Hour",color='blue')
plt.ylabel("Transaction Volume",color='blue')
plt.show()

"""##### 2. What is/are the insight(s) found from the chart?

*   From the above graph we can say that most numbers of purchasing is done between 12pm clock to 3pm.

#### Chart - 14 - Correlation Heatmap
"""

# Correlation Heatmap visualization code
numeric_df = df[df.describe().columns] # Numeric columns
correlation_matrix = numeric_df.corr()  # Correlation matrix of numeric data

sns.heatmap(correlation_matrix, cmap='coolwarm', annot = True)
plt.show()

"""##### 1. Why did you pick the specific chart?

Correlation coefficients quantify the strength and direction of the linear relationship between two variables. By incorporating correlation values into a heatmap, you can quickly see which variables are positively correlated, negatively correlated or not correlated at all.

Heatmaps visually represent data using color gradients, making it easier to spot patterns and trends.

## ***5. Hypothesis Testing***

### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing.

Answer Here.

### Hypothetical Statement - 1

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

Null Hypothesis (H0): There is no significant difference in the average sales between Thursdays and Wednesdays compared to the average sales on all other days of the week combined.

Alternative Hypothesis (H1): There is a significant difference in the average sales between Thursdays and Wednesdays compared to the average sales on all other days of the week combined.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value
day_df1 = df[df['Day'].isin(['Wednesday', 'Thursday'])][['TotalAmount']]
day_df2 = df[~df['Day'].isin(['Wednesday', 'Thursday'])][['TotalAmount']]

# applying ttest
t_statistic, p_value = stats.ttest_ind(day_df1, day_df2, equal_var=False)

print(f"t_statistic: {t_statistic}")
print(f"p_value: {p_value}")

# significance level(alpha)
alpha = 0.05

#Analyze the results
if p_value < alpha:
  print("Reject Null Hypothesis")
else:
  print("Fail to Reject Null Hypothesis")

"""##### Which statistical test have you done to obtain P-Value?

I have perform t-test to obtain P-Value.

##### Why did you choose the specific statistical test?

A t-test is a statistical method well-suited to determining if a significant difference exists between the average sales on Thursdays and Wednesdays compared to the average sales on all other weekdays combined.

### Hypothetical Statement - 2

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

Null Hypothesis (H0): There is no significant difference in the average sales between December, November and October compared to the average sales on all other months.

Alternative Hypothesis (H1): There is a significant difference in the average sales between December, November and October compared to the average sales on all other months.Answer Here.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value
day_df1 = df[df['Month'].isin(['December', 'November','October'])][['TotalAmount']]
day_df2 = df[~df['Month'].isin(['December', 'November','October'])][['TotalAmount']]

# applying ttest
t_statistic, p_value = stats.ttest_ind(day_df1, day_df2, equal_var=False)

print(f"t_statistic: {t_statistic}")
print(f"p_value: {p_value}")

# significance level(alpha)
alpha = 0.05

#Analyze the results
if p_value < alpha:
  print("Reject Null Hypothesis")
else:
  print("Fail to Reject Null Hypothesis")

"""##### Which statistical test have you done to obtain P-Value?

I have perform t-test to obtain P-Value.

##### Why did you choose the specific statistical test?

A t-test is a statistical method well-suited to determining if a significant difference exists between the average sales on December, November and October compared to the average sales on all other months combined.

### Hypothetical Statement - 3

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

Null Hypothesis (H0): There is no significant difference in the average quantity purchased per invoice between different customers.

Alternative Hypothesis (H1): There is a significant difference in the average quantity purchased per invoice between different customers.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value
# Group by customerId and invoiceid to get average quantity per invoice for each customer
df_grouped = df.groupby(['CustomerID', 'InvoiceNo']).agg({'Quantity': 'mean'}).reset_index()

# Extract necessary columns
customer_quantities = df_grouped[['CustomerID', 'Quantity']]

# Create a list of arrays for each customer's quantities
quantities_per_customer = [group['Quantity'].values for name, group in customer_quantities.groupby('CustomerID')]

# Perform one-way ANOVA
f_stat, p_value = stats.f_oneway(*quantities_per_customer)

print(f"F-statistic: {f_stat}")
print(f"P-value: {p_value}")

# Interpretation
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: There is a significant difference in the average quantity purchased per invoice between different customers.")
else:
    print("Fail to reject the null hypothesis: There is no significant difference in the average quantity purchased per invoice between different customers.")

"""##### Which statistical test have you done to obtain P-Value?

I have performs a one-way ANOVA to test the null hypothesis that there is significant difference in the average quantity purchased per invoice between different customers.

Answer Here.

## ***6. Feature Engineering & Data Pre-processing***

### 1. Handling Missing Values
"""

# Handling Missing Values & Missing Value Imputation

"""#### What all missing value imputation techniques have you used and why did you use those techniques?

I have dropped the null value.

### 2. Handling Outliers
"""

# Handling Outliers & Outlier treatments

"""##### What all outlier treatment techniques have you used and why did you use those techniques?

Answer Here.

### 3. Categorical Encoding
"""

# Encode your categorical columns

"""#### What all categorical encoding techniques have you used & why did you use those techniques?

Answer Here.

### 4. Textual Data Preprocessing
(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)

#### 1. Expand Contraction
"""

# Expand Contraction

"""#### 2. Lower Casing"""

# Lower Casing

"""#### 3. Removing Punctuations"""

# Remove Punctuations

"""#### 4. Removing URLs & Removing words and digits contain digits."""

# Remove URLs & Remove words and digits contain digits

"""#### 5. Removing Stopwords & Removing White spaces"""

# Remove Stopwords

# Remove White spaces

"""#### 6. Rephrase Text"""

# Rephrase Text

"""#### 7. Tokenization"""

# Tokenization

"""#### 8. Text Normalization"""

# Normalizing Text (i.e., Stemming, Lemmatization etc.)

"""##### Which text normalization technique have you used and why?

Answer Here.

#### 9. Part of speech tagging
"""

# POS Taging

"""#### 10. Text Vectorization"""

# Vectorizing Text

"""##### Which text vectorization technique have you used and why?

Answer Here.

### 4. Feature Manipulation & Selection

#### 1. Feature Manipulation
"""

# Manipulate Features to minimize feature correlation and create new features

"""#### 2. Feature Selection"""

# Select your features wisely to avoid overfitting

"""##### What all feature selection methods have you used  and why?

Answer Here.

##### Which all features you found important and why?

Answer Here.

### 5. Data Transformation

#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?
"""

# Transform Your data

"""### 6. Data Scaling"""

# Scaling your data

"""##### Which method have you used to scale you data and why?

### 7. Dimesionality Reduction

##### Do you think that dimensionality reduction is needed? Explain Why?

Answer Here.
"""

# DImensionality Reduction (If needed)

"""##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)

Answer Here.

### 8. Data Splitting
"""

# Split your data to train and test. Choose Splitting ratio wisely.

"""##### What data splitting ratio have you used and why?

Answer Here.

### 9. Handling Imbalanced Dataset

##### Do you think the dataset is imbalanced? Explain Why.

Answer Here.
"""

# Handling Imbalanced Dataset (If needed)

"""##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)

Answer Here.

## ***7. ML Model Implementation***

### ML Model - 1 Create the RFM model (Recency, Frequency,Monetary value)
"""

# ML Model - 1 Implementation

#Set Latest date 2011-12-10 as last invoice date was 2011-12-09. This is to calculate the number of days from recent purchase
Latest_Date = dt.datetime(2011,12,10)
#Create RFM Modelling scores for each customer
rfm_df = df.groupby('CustomerID').agg({'InvoiceDate': lambda x: (Latest_Date - x.max()).days,
                                       'InvoiceNo': lambda x: len(x),
                                       'TotalAmount': lambda x: x.sum()})

# Handle non-finite values before converting to integer
rfm_df['InvoiceDate'] = rfm_df['InvoiceDate'].fillna(0).astype(int) # Replace NaN with 0, then convert to int

#Rename column names to Recency, Frequency and Monetary
rfm_df.rename(columns={'InvoiceDate': 'Recency',
                         'InvoiceNo': 'Frequency',
                         'TotalAmount': 'Monetary'}, inplace=True)

rfm_df.reset_index().head()

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.

RFM is a method used for analyzing customer value. It is commonly used in database marketing and direct marketing and has received particular attention in retail and professional services industries.

Recency – How recently did the customer purchase?

Frequency – How often do they purchase?

Monetary – How much do they spend?

Recency - In order to find the recency value of each customer, we need to determine the last invoice date as the current date and subtract the last purchasing date of each customer from this date.

Frequency - In order to find the frequency value of each customer, we need to determine how many times the customers make purchases.

Monetary - In order to find the monetary value of each customer, we need to determine how much do the customers spend on purchases.
"""

#Descriptive Statistics
rfm_df.describe().T

# Distribution plot
# plots size
fig, axes = plt.subplots(1,3,figsize=(20,5))
sns.histplot(rfm_df['Recency'], ax = axes[0],color='gray', kde=True)  # Recency Distribution
axes[0].set_title('Recency Distribution')

sns.histplot(rfm_df['Frequency'], ax = axes[1],color='brown', kde=True)  # Frequency Distribution
axes[1].set_title('Frequency Distribution')

sns.histplot(rfm_df['Monetary'], ax = axes[2],color='steelblue', kde=True)  # Monetary Distribution
axes[2].set_title('Monetary Distribution')

plt.show()  # showing graph

#Split into four segments using quantiles
quantiles = rfm_df.quantile(q=[0.25,0.5,0.75])
quantiles = quantiles.to_dict()
quantiles

'''Functions to create R, F and M segments according to quantiles for Recency low score is important and for Frequency and Monetory
maximum is important. So keeping this in mind we are creating two function to create scores. '''
def RScoring(x,p,d):
    if x <= d[p][0.25]:
        return 1
    elif x <= d[p][0.50]:
        return 2
    elif x <= d[p][0.75]:
        return 3
    else:
        return 4

def FnMScoring(x,p,d):
    if x <= d[p][0.25]:
        return 4
    elif x <= d[p][0.50]:
        return 3
    elif x <= d[p][0.75]:
        return 2
    else:
        return 1

#Calculate Add R, F and M segment value columns in the existing dataset to show R, F and M segment values
rfm_df['R'] = rfm_df['Recency'].apply(RScoring, args=('Recency',quantiles))
rfm_df['F'] = rfm_df['Frequency'].apply(FnMScoring, args=('Frequency',quantiles))
rfm_df['M'] = rfm_df['Monetary'].apply(FnMScoring, args=('Monetary',quantiles))
rfm_df.head()

#Calculate and Add RFMGroup value column showing combined concatenated score of RFM
rfm_df['RFMGroup'] = rfm_df.R.map(str) + rfm_df.F.map(str) + rfm_df.M.map(str)

#Calculate and Add RFMScore value column showing total sum of RFMGroup values
rfm_df['RFMScore'] = rfm_df[['R', 'F', 'M']].sum(axis = 1)
rfm_df.head()

"""Note: Customers with an RFM score of 111 are typically our most engaged customers, while those with a score of 444 are at risk of churn."""

#Handling negative and zero values so as to handle infinite numbers during log transformation
def handle_neg_n_zero(num):
    if num <= 0:
        return 1
    else:
        return num
#Applying handle_neg_n_zero function to Recency and Monetary columns
rfm_df['Recency'] = [handle_neg_n_zero(x) for x in rfm_df.Recency]
rfm_df['Monetary'] = [handle_neg_n_zero(x) for x in rfm_df.Monetary]

#Performing Log transformation to bring data into normal or near normal distribution
Log_Tfd_Data = rfm_df[['Recency', 'Frequency', 'Monetary']].apply(np.log, axis = 1).round(3)

# Data Distribution after log transformation
fig, axes = plt.subplots(1,3,figsize=(20,5))
sns.histplot(Log_Tfd_Data['Recency'], ax = axes[0],color='gray', kde=True)  # Recency Distribution
axes[0].set_title('Recency Distribution')

sns.histplot(Log_Tfd_Data['Frequency'], ax = axes[1],color='brown', kde=True)  # Frequency Distribution
axes[1].set_title('Frequency Distribution')

sns.histplot(Log_Tfd_Data['Monetary'], ax = axes[2],color='steelblue', kde=True)  # Monetary Distribution
axes[2].set_title('Monetary Distribution')

plt.show()

# Log Transform our Recency, Frequency and Monetary column and store it in new features.
rfm_df['Recency_log'] = rfm_df['Recency'].apply(math.log)
rfm_df['Frequency_log'] = rfm_df['Frequency'].apply(math.log)
rfm_df['Monetary_log'] = rfm_df['Monetary'].apply(math.log)

"""### ML Model - 2  **K Means Clustering**

Clustering is an unsupervised classification techinque to understand the groups of classes in the data. We use the K-means clustering algorithm to determine the ideal segments of customers.


KMeans requires the number of clusters to be specified during the model building process. To know the right number of clusters, methods such as silhouette analysis and elbow method can be used. These methods will help in selection of the optimum number of clusters.

Silhouette score method:

Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other. The Silhouette score is calculated for each sample of different clusters.

# Applying Silhouette Score Method on Recency and Monetary
"""

#silhoutte score
features_rec_mon = ['Recency_log','Monetary_log']
X_features_rec_mon = rfm_df[features_rec_mon].values

scaler_rec_mon = StandardScaler()
X_rec_mon = scaler_rec_mon.fit_transform(X_features_rec_mon)   # scaling our features
X_rm = X_rec_mon

range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]     # different number of cluster
for n_clusters in range_n_clusters:
    clusterer = KMeans(n_clusters=n_clusters)       # passing different number of cluster
    preds = clusterer.fit_predict(X_rm)
    centers = clusterer.cluster_centers_

    score = silhouette_score(X_rm, preds)             # calculating silhouette score
    print("For n_clusters = {}, silhouette score is {}".format(n_clusters, score))  # printing number of cluster and silhouette score

"""# Applying Elbow Method on Recency and Monetary:

Elbow Method:

Elbow is one of the most famous methods by which you can select the right value of k and boost your model performance. We also perform the hyperparameter tuning to chose the best value of k. It is an empirical method to find out the best value of k. it picks up the range of values and takes the best among them. It calculates the sum of the square of the points and calculates the average distance.
"""

#applying elbow method

sum_of_sq_dist = {}
for k in range(1,15):
    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)
    km = km.fit(X_rm)
    sum_of_sq_dist[k] = km.inertia_

#Plot the graph for the sum of square distance values and Number of Clusters
plt.figure(figsize=(12,8))
sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()),color='r')
plt.xlabel('Number of Clusters(k)',color='blue')
plt.ylabel('Sum of Square Distances', color= 'blue')
plt.title('Elbow Method For Optimal k', color = 'navy')
plt.show()

"""# Both the silhouette score and the elbow method suggest that a 2-cluster solution may be optimal for this data."""

kmeans = KMeans(n_clusters=2)
kmeans.fit(X_rm)
y_kmeans= kmeans.predict(X_rm)
# vizualize two cluster
plt.figure(figsize=(12,8))  # figure size
plt.title('customer segmentation based on Recency and Monetary',color='navy')
plt.scatter(X_rm[:, 0], X_rm[:, 1], c=y_kmeans, s=50, cmap='spring_r')

centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)

"""# Applying silhouette Score Method on Frquency and Monetary"""

#silhouetee score
features_fre_mon = ['Frequency_log','Monetary_log']
X_features_fre_mon = rfm_df[features_fre_mon].values

scaler_fre_mon = StandardScaler()
X_fre_mon = scaler_fre_mon.fit_transform(X_features_fre_mon)
X_fm = X_fre_mon

range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]
for n_clusters in range_n_clusters:
    clusterer = KMeans(n_clusters=n_clusters)
    preds = clusterer.fit_predict(X_fm)
    centers = clusterer.cluster_centers_

    score = silhouette_score(X_fm, preds)
    print("For n_clusters = {}, silhouette score is {}".format(n_clusters, score))

"""# Applying Elbow Method on Frequency and Monetary"""

#applying elbow method
sum_of_sq_dist = {}
for k in range(1,15):
    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)
    km = km.fit(X_fm)
    sum_of_sq_dist[k] = km.inertia_

#Plot the graph for the sum of square distance values and Number of Clusters
plt.figure(figsize=(10,6))
sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()),color='r')
plt.xlabel('Number of Clusters(k)')
plt.ylabel('Sum of Square Distances')
plt.title('Elbow Method For Optimal k')
plt.show()

kmeans = KMeans(n_clusters=2)
kmeans.fit(X_fm)
y_kmeans= kmeans.predict(X_fm)

#plotting graph based on frequency and monetary
plt.figure(figsize=(15,10))
plt.title('customer segmentation based on Frequency and Monetary')
plt.scatter(X_fm[:, 0], X_fm[:, 1], c=y_kmeans, s=50, cmap='PRGn')

centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)

"""# Applying Elbow Method on Recency, Frequency and Monetary."""

sum_of_sq_dist = {}
for k in range(1,15):
    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)
    km = km.fit(X)
    sum_of_sq_dist[k] = km.inertia_

#Plot the graph for the sum of square distance values and Number of Clusters
plt.figure(figsize=(8,6))
sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()),color='r')
plt.xlabel('Number of Clusters(k)')
plt.ylabel('Sum of Square Distances')
plt.title('Elbow Method For Optimal k')
plt.show()

"""##### Which hyperparameter optimization technique have you used and why?

Unsupervised learning helps us group customers with similar traits for targeted marketing campaigns. To find the best groups, I used silhouette and elbow methods, which are like tools to check how well data points are grouped together. In both cases, the results suggested two groups as the best fit.

In unsupervised learning, particularly with K-Means clustering, determining the optimal number of clusters is essential. The elbow and silhouette methods are instrumental in identifying this "sweet spot" for the number of clusters.

### ML Model - 3

DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. DBSCAN is a density-based clustering algorithm that works on the assumption that clusters are dense regions in space separated by regions of lower density.It groups ‘densely grouped’ data points into a single cluster.

## Applying DBSCAN on Recency and Monetary
"""

# ML Model - 3 Implementation
y_pred = DBSCAN(eps=0.5, min_samples=15).fit_predict(X_rm)
plt.figure(figsize=(13,8))
plt.scatter(X_rm[:,0], X_rm[:,1], c=y_pred,cmap='spring_r')

"""# Applying DBSCAN Method on Frquency and Monetary."""

y_pred = DBSCAN(eps=0.5, min_samples=15).fit_predict(X_fm)
plt.figure(figsize=(13,8))
plt.scatter(X_fm[:,0], X_fm[:,1], c=y_pred)

"""# Applying DBSCAN to Recency, Frequency and Monetary."""

y_pred = DBSCAN(eps=0.5, min_samples=15).fit_predict(X)
plt.figure(figsize=(13,8))
plt.scatter(X[:,0], X[:,1], c=y_pred)

"""#### 3. Explain the model which you have used."""

#Performing K-Means Clustering with 2 clusters
KMean_clust = KMeans(n_clusters= 2, init= 'k-means++', max_iter= 1000)
KMean_clust.fit(X)

#Find the clusters for the observation given in the dataset
rfm_df['Cluster'] = KMean_clust.labels_
#First 10 rows of the RFM dataframe
rfm_df.head(10)

#let's check mean values of the cluster for recency, frequnecy and monetary

rfm_df.groupby('Cluster').agg({'Recency':'mean',
                               'Frequency':'mean',
                               'Monetary':'mean'})

from prettytable import PrettyTable

# Specify the Column Names while initializing the Table
myTable = PrettyTable(['Sr No.',"Model_Name",'Data', "Optimal_Number_of_cluster"])

# Add rows
myTable.add_row(['1',"K-Means with silhouette_score ", "RM", "2"])
myTable.add_row(['2',"K-Means with Elbow methos  ", "RM", "2"])
myTable.add_row(['3',"DBSCAN ", "RM", "2"])
myTable.add_row(['4',"K-Means with silhouette_score ", "FM", "2"])
myTable.add_row(['5',"K-Means with Elbow methos  ", "FM", "2"])
myTable.add_row(['6',"DBSCAN ", "FM", "2"])
myTable.add_row(['7',"K-Means with silhouette_score ", "RFM", "2"])
myTable.add_row(['8',"K-Means with Elbow methos  ", "RFM", "2"])
myTable.add_row(['9',"DBSCAN ", "RFM", "3"])

print(myTable)

"""# **Conclusion**

*   This project focuses on developing customer segments for a UK-based online store that sells unique, all-occasion gifts.

*   Using a recency, frequency, and monetary (RFM) analysis, customers were segmented into various clusters. The analysis resulted in a silhouette score of 0.39 for two clusters.

*   By applying different clustering algorithms to the dataset, it was determined that the optimal number of clusters is two.

*   The business can now focus on these distinct clusters and tailor its services to each segment accordingly. This targeted approach will benefit both the customers and the business, leading to enhanced customer satisfaction and overall business performance.

# **Hurrah! You have successfully completed your Machine Learning Capstone Project !!!**
"""